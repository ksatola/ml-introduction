{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You’ll start by building the simplest imaginable (unvalidated) POI identifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import sys\n",
    "from time import time\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'salary': 365788,\n",
       " 'to_messages': 807,\n",
       " 'deferral_payments': 'NaN',\n",
       " 'total_payments': 1061827,\n",
       " 'loan_advances': 'NaN',\n",
       " 'bonus': 600000,\n",
       " 'email_address': 'mark.metts@enron.com',\n",
       " 'restricted_stock_deferred': 'NaN',\n",
       " 'deferred_income': 'NaN',\n",
       " 'total_stock_value': 585062,\n",
       " 'expenses': 94299,\n",
       " 'from_poi_to_this_person': 38,\n",
       " 'exercised_stock_options': 'NaN',\n",
       " 'from_messages': 29,\n",
       " 'other': 1740,\n",
       " 'from_this_person_to_poi': 1,\n",
       " 'poi': False,\n",
       " 'long_term_incentive': 'NaN',\n",
       " 'shared_receipt_with_poi': 702,\n",
       " 'restricted_stock': 585062,\n",
       " 'director_fees': 'NaN'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict = pickle.load(open(\"final_project_dataset.pkl\", \"rb\"))\n",
    "print(len(data_dict))\n",
    "#next(iter(data_dict.values()))\n",
    "data_dict.get('METTS MARK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### first element is our labels, any added elements are predictor\n",
    "### features. Keep this the same for the mini-project, but you'll\n",
    "### have a different feature list when you do the final project.\n",
    "features_list = [\"poi\", \"salary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = featureFormat(data_dict, features_list, sort_keys = 'python2_lesson13_keys.pkl')\n",
    "# The sort_keys is only used to have the same grader result as in Python 2\n",
    "labels, features = targetFeatureSplit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a decision tree classifier (just use the default parameters), train it on all the data (you will fix this in the next part!), and print out the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.6578800e+05]\n",
      " [2.6710200e+05]\n",
      " [1.7094100e+05]\n",
      " [2.4329300e+05]\n",
      " [2.6709300e+05]\n",
      " [3.7044800e+05]\n",
      " [1.9709100e+05]\n",
      " [1.3072400e+05]\n",
      " [2.8858900e+05]\n",
      " [2.4854600e+05]\n",
      " [2.5748600e+05]\n",
      " [2.8854200e+05]\n",
      " [2.5165400e+05]\n",
      " [2.8855800e+05]\n",
      " [6.3744000e+04]\n",
      " [3.5709100e+05]\n",
      " [2.7144200e+05]\n",
      " [3.0411000e+05]\n",
      " [1.8792200e+05]\n",
      " [2.1362500e+05]\n",
      " [2.4920100e+05]\n",
      " [2.3133000e+05]\n",
      " [1.8224500e+05]\n",
      " [2.1178800e+05]\n",
      " [2.2430500e+05]\n",
      " [2.7374600e+05]\n",
      " [3.3928800e+05]\n",
      " [2.1658200e+05]\n",
      " [2.1050000e+05]\n",
      " [2.7288000e+05]\n",
      " [4.7700000e+02]\n",
      " [2.6907600e+05]\n",
      " [4.2878000e+05]\n",
      " [2.1184400e+05]\n",
      " [2.0612100e+05]\n",
      " [1.7424600e+05]\n",
      " [5.1036400e+05]\n",
      " [3.6503800e+05]\n",
      " [3.6516300e+05]\n",
      " [1.6277900e+05]\n",
      " [2.3645700e+05]\n",
      " [1.0723210e+06]\n",
      " [2.6151600e+05]\n",
      " [3.2907800e+05]\n",
      " [1.8489900e+05]\n",
      " [1.9200800e+05]\n",
      " [2.6341300e+05]\n",
      " [2.6266300e+05]\n",
      " [3.7412500e+05]\n",
      " [2.7860100e+05]\n",
      " [1.9915700e+05]\n",
      " [9.6840000e+04]\n",
      " [8.0818000e+04]\n",
      " [2.1399900e+05]\n",
      " [2.1069200e+05]\n",
      " [2.2209300e+05]\n",
      " [4.4069800e+05]\n",
      " [2.4018900e+05]\n",
      " [4.2063600e+05]\n",
      " [2.7510100e+05]\n",
      " [3.1428800e+05]\n",
      " [9.4941000e+04]\n",
      " [2.3950200e+05]\n",
      " [1.1112580e+06]\n",
      " [6.6150000e+03]\n",
      " [6.5503700e+05]\n",
      " [4.0433800e+05]\n",
      " [2.6704229e+07]\n",
      " [2.5999600e+05]\n",
      " [3.1754300e+05]\n",
      " [2.0195500e+05]\n",
      " [2.4814600e+05]\n",
      " [7.6399000e+04]\n",
      " [2.6278800e+05]\n",
      " [2.6180900e+05]\n",
      " [2.4801700e+05]\n",
      " [2.2928400e+05]\n",
      " [2.3194600e+05]\n",
      " [2.2100300e+05]\n",
      " [1.5840300e+05]\n",
      " [2.5010000e+05]\n",
      " [4.9237500e+05]\n",
      " [1.0609320e+06]\n",
      " [2.6187900e+05]\n",
      " [2.3967100e+05]\n",
      " [3.0458800e+05]\n",
      " [3.0994600e+05]\n",
      " [8.5274000e+04]\n",
      " [2.4733800e+05]\n",
      " [3.4948700e+05]\n",
      " [3.3054600e+05]\n",
      " [4.1518900e+05]\n",
      " [2.6521400e+05]\n",
      " [2.7860100e+05]\n",
      " [2.7497500e+05]]\n"
     ]
    }
   ],
   "source": [
    "print(np.array(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(np.array(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 0.006 seconds.\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "# Measure time\n",
    "t0 = time()\n",
    "\n",
    "# Fit the model\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf.fit(features, labels)\n",
    "\n",
    "print(\"Training time: {} seconds.\".format(round(time()-t0, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 0.001 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Measure time\n",
    "t0 = time()\n",
    "\n",
    "# Predict\n",
    "pred = clf.predict(features)\n",
    "\n",
    "print(\"Training time: {} seconds.\".format(round(time()-t0, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9894736842105263"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(pred, labels)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you’ll add in training and testing, so that you get a trustworthy accuracy number.\n",
    "1. Use the train_test_split validation available in sklearn.cross_validation; \n",
    "2. Hold out 30% of the data for testing and set the random_state parameter to 42 (random_state controls which points go into the training set and which are used for testing). \n",
    "\n",
    "What’s your updated accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 0.002 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Measure time\n",
    "t0 = time()\n",
    "\n",
    "# Fit the model\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf.fit(features_train, labels_train)\n",
    "\n",
    "print(\"Training time: {} seconds.\".format(round(time()-t0, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 0.001 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Measure time\n",
    "t0 = time()\n",
    "\n",
    "# Predict\n",
    "pred_test = clf.predict(features_test)\n",
    "\n",
    "print(\"Training time: {} seconds.\".format(round(time()-t0, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7241379310344828"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(pred_test, labels_test)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many POIs are predicted for the test set for your POI identifier? (Note that we said test set! We are not looking for the number of POIs in the whole dataset.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 1. 1. 0. 1. 0.]\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(np.array(labels_test))\n",
    "print(len([e for e in labels_test if e == 1.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many people total are in your test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your identifier predicted 0. (not POI) for everyone in the test set, what would its accuracy be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8620689655172413\n"
     ]
    }
   ],
   "source": [
    "print(1. - (4./29.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the predictions of your model and compare them to the true test labels. Do you get any true positives? (In this case, we define a true positive as a case where both the actual label and the predicted label are 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         poi       0.84      0.84      0.84        25\n",
      "      salary       0.00      0.00      0.00         4\n",
      "\n",
      "   micro avg       0.72      0.72      0.72        29\n",
      "   macro avg       0.42      0.42      0.42        29\n",
      "weighted avg       0.72      0.72      0.72        29\n",
      "\n",
      "[[21  4]\n",
      " [ 4  0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "n_classes = len(features_list)\n",
    "print(\"Number of classes: %d\" % n_classes)\n",
    "print(classification_report(labels_test, pred_test, target_names=features_list))\n",
    "print(confusion_matrix(labels_test, pred_test, labels=range(n_classes)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you may now see, having imbalanced classes like we have in the Enron dataset (many more non-POIs than POIs) introduces some special challenges, namely that you can just guess the more common class label for every point, not a very insightful strategy, and still get pretty good accuracy!\n",
    "\n",
    "Precision and recall can help illuminate your performance better. Use the precision_score and recall_score available in sklearn.metrics to compute those quantities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the precision?\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# What is the precision?\n",
    "print(\"What is the precision?\")\n",
    "from sklearn.metrics import *\n",
    "print(precision_score(labels_test, clf.predict(features_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Note: you may see a message like UserWarning: The precision and recall are equal to zero for some labels. Just like the message says, there can be problems in computing other metrics (like the F1 score) when precision and/or recall are zero, and it wants to warn you when that happens.) \n",
    "\n",
    "Obviously this isn’t a very optimized machine learning strategy (we haven’t tried any algorithms besides the decision tree, or tuned any parameters, or done any feature selection), and now seeing the precision and recall should make that much more apparent than the accuracy did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the recall?\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# What is the recall?\n",
    "print(\"What is the recall?\")\n",
    "print(recall_score(labels_test, clf.predict(features_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the final project you’ll work on optimizing your POI identifier, using many of the tools learned in this course. Hopefully one result will be that your precision and/or recall will go up, but then you’ll have to be able to interpret them. \n",
    "\n",
    "Here are some made-up predictions and true labels for a hypothetical test set; fill in the following boxes to practice identifying true positives, false positives, true negatives, and false negatives. Let’s use the convention that “1” signifies a positive result, and “0” a negative. \n",
    "\n",
    "predictions = [0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1] \n",
    "true labels = [0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666666666666666\n",
      "0.75\n",
      "[[9 3]\n",
      " [2 6]]\n"
     ]
    }
   ],
   "source": [
    "predictions = [0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1]\n",
    "true_labels = [0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0]\n",
    "print(precision_score(true_labels, predictions))\n",
    "print(recall_score(true_labels, predictions))\n",
    "print(confusion_matrix(true_labels, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many true positives are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many true negatives are there in this example?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many false positives are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many false negatives are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the precision of this classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.6666666666666666"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the recall of this classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
